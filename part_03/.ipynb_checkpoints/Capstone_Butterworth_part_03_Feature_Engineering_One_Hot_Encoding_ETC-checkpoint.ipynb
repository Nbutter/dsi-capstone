{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering With Reduced Dataset and Class Rebalancing\n",
    "\n",
    "(A revised version of this notebook where we allow FeatureHasher to pick the num_features instead of settting them directly)\n",
    "\n",
    "(We also add class rebalancing to make a 50/50 ratio)\n",
    "\n",
    "Since feature engineering for categorical variables (eg one hot encoding) is difficult with our full 6GB dataset on our local computer (8GB RAM), in this notebook we load ~25% of our training data (~10MM rows) to explore different techniques, such as Feature Hashing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction import FeatureHasher \n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report, log_loss, roc_auc_score, roc_curve\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Data and Sample Submission\n",
    "\n",
    "Loading in our full test file as well as the sample submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in test data as well as the Sample Submission file\n",
    "sample = pd.read_csv('../assets/sampleSubmission')\n",
    "test = pd.read_csv('../assets/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Partial Training Data\n",
    "\n",
    "For this notebook we are only loading 20% of our full training dataset (~1.5GB, ~10MM rows). This will allow us to experiment with feature engineering on a local device with 8GB of memory. Running the same operations against the full set may require a different solution (eg AWS Sagemaker or Databricks Spark Cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.2 s, sys: 9.67 s, total: 49.8 s\n",
      "Wall time: 51.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "      <th>hour</th>\n",
       "      <th>C1</th>\n",
       "      <th>banner_pos</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_domain</th>\n",
       "      <th>site_category</th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>device_type</th>\n",
       "      <th>device_conn_type</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000009e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>1fbe01fe</td>\n",
       "      <td>f3845767</td>\n",
       "      <td>28905ebd</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15706</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>1722</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  click      hour    C1  banner_pos   site_id site_domain  \\\n",
       "0  1.000009e+18      0  14102100  1005           0  1fbe01fe    f3845767   \n",
       "\n",
       "  site_category    app_id app_domain ...  device_type device_conn_type    C14  \\\n",
       "0      28905ebd  ecad2386   7801e8d9 ...            1                2  15706   \n",
       "\n",
       "   C15  C16   C17  C18  C19  C20  C21  \n",
       "0  320   50  1722    0   35   -1   79  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because of GitHub space limits (no files over 2GB), train data file was split into 5 pieces\n",
    "\n",
    "# Loading the first file with header row to use for column names\n",
    "%time train = pd.read_csv(\"../assets/trainaa\")\n",
    "\n",
    "# Checking the columns present\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance\n",
    "We have only 16% minority class representation in this reduced dataset. Let's see whether changing this balance results in better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.833864\n",
       "1    0.166136\n",
       "Name: click, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.click.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will downsample the majority class to be the same size as the minority class in order to test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8338642, 24), (1661357, 24))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting training dataset into minority and majority classes so we can downsample majority class\n",
    "train_0 = train[train.click == 0]\n",
    "train_1 = train[train.click == 1]\n",
    "(train_0.shape, train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.37 s, sys: 1.8 s, total: 5.17 s\n",
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "# Reducing the majority class to be the same size as the minority class\n",
    "%time train_0_reduced = resample(train_0, replace=False, n_samples=train_1.shape[0], random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the reduced majority class to our minority class to make a new training set\n",
    "train = train_0_reduced.append(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3322714, 24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have reduced down to about 1/3 of our original dataset size\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: click, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can see that each class accounts for about 50% of the rows in our dataset\n",
    "train.click.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a 50/50 balance between classes in our training set. Let's see whether we get different results in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Y Train\n",
    "We'll use the \"click\" column in the training set as our target or \"y\" series for training purposes, dropping it from our training dataframe.  We will also remove the \"id\" column, which does not add value for training purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a y_train with our click information\n",
    "y_train = train.click\n",
    "\n",
    "# From the Kaggle site, we know 'id' is just a record indicator, so we can drop it\n",
    "# along with the \"click\" column, which is our target\n",
    "train = train.drop(columns=['click', 'id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3322714, 22), (4577464, 23), (3322714,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking our shapes. We see that \"test\" has an extra column, because it still has \"id\". We'll drop this column next \n",
    "# after using it to prepare a submission dataframe, where we'll put our predictions\n",
    "(train.shape, test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: click, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class balancce in y_train\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping a Submission Dataframe\n",
    "We take the \"id\" column and index from the \"test\" dataframe and add a \"click\" column, following the model of the sample submission file.  For now the click column is filled with zeros; later as we generate model predictions we will place them here, before saving this dataframe out as a csv to upload to Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000174058809263569</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  click\n",
       "0  10000174058809263569    0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the columns of our submission sample\n",
    "sample.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        uint64\n",
       "click    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the datatypes of our submission sample\n",
    "sample.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        uint64\n",
       "click    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the submission dataframe and conforming types\n",
    "submit = pd.DataFrame(test.id, index=test.index)\n",
    "submit['click'] = 0.0\n",
    "submit['id'] = submit['id'].astype(np.uint64)\n",
    "\n",
    "# Verifying that our new submission has the correct datatypes\n",
    "submit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000174058809264128</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  click\n",
       "0  10000174058809264128    0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that our new submission has the correct columns\n",
    "submit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3322714, 22), (4577464, 22), (4577464, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the id column from test\n",
    "test = test.drop(columns=\"id\")\n",
    "\n",
    "# Now we can verify that \"train\" and \"test\" have the same number of columns, as expected\n",
    "# We can also verify that the \"submit\" dataframe has the same number of rows as \"test\"\n",
    "\n",
    "(train.shape, test.shape, submit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Date Columns\n",
    "We have a column representing hour/day/month/year for each hour in our 14 day sample (\"hour\"). We will convert this to a date-time object, then add feature columns based on the day of the week and the hour of the day.  Finally we will remove the hour column. We'll perform the same operation on both our \"train\" and \"test\" dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make day-of-week, hour-of-day features\n",
    "def make_date_features(dataframe, frame_name=\"dataframe\", date_col=\"hour\"): \n",
    "    date_obj = pd.to_datetime(dataframe[date_col])\n",
    "    \n",
    "    dataframe['hour-of-the-day'] = date_obj.dt.hour\n",
    "    print(f\"Created 'hour-of-the-day' column in {frame_name}\")\n",
    "    \n",
    "    dataframe['day-of-the-week'] = date_obj.dt.dayofweek\n",
    "    print(f\"Created 'day-of-the-week' column in {frame_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'hour-of-the-day' column in train\n",
      "Created 'day-of-the-week' column in train\n",
      "Created 'hour-of-the-day' column in test\n",
      "Created 'day-of-the-week' column in test\n"
     ]
    }
   ],
   "source": [
    "# running \"train\" and \"test\" through our date features function to create the engineered columns \n",
    "make_date_features(train, \"train\", \"hour\")\n",
    "make_date_features(test, \"test\", \"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the \"hour\" column now that we no longer need it\n",
    "train = train.drop(columns=[\"hour\"])\n",
    "test = test.drop(columns=[\"hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from Adam - Standup July 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator object from readcsv - ton of cool features - get batches\n",
    "# hashing vectorizer\n",
    "# stochastic sgd\n",
    "# no SVM - no KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Hash Experiment\n",
    "\n",
    "We have multiple columns with large numbers of categorical values. Let's see if we can featureize them in memory with the FeatureHash class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>device_ip</td>\n",
       "      <td>1128197</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device_id</td>\n",
       "      <td>361531</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>device_model</td>\n",
       "      <td>6043</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>app_id</td>\n",
       "      <td>4252</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>site_domain</td>\n",
       "      <td>3464</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>site_id</td>\n",
       "      <td>2964</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C14</td>\n",
       "      <td>1002</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>app_domain</td>\n",
       "      <td>299</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C17</td>\n",
       "      <td>222</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C20</td>\n",
       "      <td>166</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C19</td>\n",
       "      <td>46</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C21</td>\n",
       "      <td>42</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>app_category</td>\n",
       "      <td>28</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>site_category</td>\n",
       "      <td>22</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C16</td>\n",
       "      <td>9</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C15</td>\n",
       "      <td>8</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>banner_pos</td>\n",
       "      <td>7</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C1</td>\n",
       "      <td>7</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>device_type</td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C18</td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>device_conn_type</td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hour-of-the-day</td>\n",
       "      <td>1</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>day-of-the-week</td>\n",
       "      <td>1</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column Name Unique Values   Dtype\n",
       "0          device_ip       1128197  object\n",
       "1          device_id        361531  object\n",
       "2       device_model          6043  object\n",
       "3             app_id          4252  object\n",
       "4        site_domain          3464  object\n",
       "5            site_id          2964  object\n",
       "6                C14          1002   int64\n",
       "7         app_domain           299  object\n",
       "8                C17           222   int64\n",
       "9                C20           166   int64\n",
       "10               C19            46   int64\n",
       "11               C21            42   int64\n",
       "12      app_category            28  object\n",
       "13     site_category            22  object\n",
       "14               C16             9   int64\n",
       "15               C15             8   int64\n",
       "16        banner_pos             7   int64\n",
       "17                C1             7   int64\n",
       "18       device_type             4   int64\n",
       "19               C18             4   int64\n",
       "20  device_conn_type             4   int64\n",
       "21   hour-of-the-day             1   int64\n",
       "22   day-of-the-week             1   int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the number of unique values in each column of the training dataset\n",
    "uniques = pd.DataFrame(data=[train.columns.values, [len(train[col].unique()) for col in train.columns], [train[col].dtype for col in train.columns]]).T\n",
    "uniques = uniques.rename({0:'Column Name', 1:'Unique Values', 2:'Dtype'}, axis=1)\n",
    "uniques = uniques.sort_values(by='Unique Values', ascending=False).reset_index(drop=True)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we are going to let FeatureHasher choose the number of features vs setting them ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.52 s, sys: 282 ms, total: 9.8 s\n",
      "Wall time: 9.81 s\n"
     ]
    }
   ],
   "source": [
    "# Let's try this out with \"device model\" first - only about 7k values\n",
    "fh_1 = FeatureHasher(input_type='string', non_negative=True) # so we can use NaiveBayes\n",
    "%time fit = fh_1.fit_transform(train.device_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit shape: (3322714, 1048576), Fit non-nulls: 21568819\n",
      "Non-null fraction of total: 0.0000061906\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fit shape: {fit.shape}, Fit non-nulls: {fit.nnz}\")\n",
    "print(f\"Non-null fraction of total: {'{:.10f}'.format(fit.nnz/(fit.shape[0] * fit.shape[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.29 s, sys: 268 ms, total: 9.55 s\n",
      "Wall time: 9.56 s\n"
     ]
    }
   ],
   "source": [
    "# Now we'll try \"device_id\" - about 700k values\n",
    "fh_2 = FeatureHasher(input_type='string', non_negative=True)\n",
    "%time fit2 = fh_2.fit_transform(train.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit2 shape: (3322714, 1048576), Fit2 non-nulls: 20220364\n",
      "Non-null fraction of total: 0.0000058036\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fit2 shape: {fit2.shape}, Fit2 non-nulls: {fit2.nnz}\")\n",
    "print(f\"Non-null fraction of total: {'{:.10f}'.format(fit2.nnz/(fit2.shape[0] * fit2.shape[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 371 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "# Now we'll try \"device_ip\" - 2MM values in the reduced training set\n",
    "fh_3 = FeatureHasher(input_type='string', non_negative=True)\n",
    "%time fit3 = fh_3.fit_transform(train.device_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit3 shape: (3322714, 1048576), Fit3 non-nulls: 21415816\n",
      "Non-null fraction of total: 0.0000061467\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fit3 shape: {fit3.shape}, Fit3 non-nulls: {fit3.nnz}\")\n",
    "print(f\"Non-null fraction of total: {'{:.10f}'.format(fit3.nnz/(fit3.shape[0] * fit3.shape[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the same objects on test now that they have been fitted on train\n",
    "fit1_test = fh_1.transform(test.device_model)\n",
    "fit2_test = fh_2.transform(test.device_id)\n",
    "fit3_test = fh_3.transform(test.device_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3322714, 1048576), (4577464, 1048576))\n",
      "((3322714, 1048576), (4577464, 1048576))\n",
      "((3322714, 1048576), (4577464, 1048576))\n"
     ]
    }
   ],
   "source": [
    "# Verifying that all our shapes are as expected\n",
    "print((fit.shape, fit1_test.shape))\n",
    "print((fit2.shape, fit2_test.shape))\n",
    "print((fit3.shape, fit3_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.64 s, sys: 361 ms, total: 8 s\n",
      "Wall time: 8.02 s\n",
      "CPU times: user 11 s, sys: 775 ms, total: 11.8 s\n",
      "Wall time: 11.9 s\n",
      "((3322714, 1048576), (4577464, 1048576))\n",
      "13914168\n"
     ]
    }
   ],
   "source": [
    "# To use with a numeric series (eg C14) we need to convert the values to strings\n",
    "# Note that casting the series as type \"object\" won't work - we need each value to be parsed as a string\n",
    "fh_4 = FeatureHasher(input_type='string', non_negative=True)\n",
    "%time fit4 = fh_4.fit_transform(train.C14.map(lambda x: str(x)))\n",
    "%time fit4_test = fh_4.transform(test.C14.map(lambda x: str(x)))\n",
    "print((fit4.shape, fit4_test.shape))\n",
    "print(fit4.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models on FeatureHash Columns\n",
    "At this point, we have used FeatureHash to convert four columns of each dataset into sparse feature matrices, with identical width (ie number of columns). Therefore we are in a position to use these features for training and predicting on a model which takes sparse features. Let's do that for demonstration purposes. \n",
    "\n",
    "If successful, we can go on to create pipelines for transforming all of our columns and move on to the modeling stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3322714, 1048576),\n",
       " (3322714, 1048576),\n",
       " (3322714, 1048576),\n",
       " (3322714, 1048576))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shapes to make sure our matrices are congruent\n",
    "(fit.shape, fit2.shape, fit3.shape, fit4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 3.62 s, total: 6.14 s\n",
      "Wall time: 6.91 s\n"
     ]
    }
   ],
   "source": [
    "# Assembling one big sparse matrix from the different HashFeature outputs\n",
    "%time train_hashed = hstack((fit, fit2, fit3, fit4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling our sparse matrix\n",
    "ss = StandardScaler(with_mean=False) # to maintain sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 s, sys: 3.84 s, total: 7.61 s\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "%time train_hashed = ss.fit_transform(train_hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3322714, 4194304), (3322714,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that we have compatible shapes of correct dimensionality\n",
    "(train_hashed.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the Multinomial Naive Bayes classifier, since each of our features may have a multinomial distribution\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 179 ms, total: 1.23 s\n",
      "Wall time: 1.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "%time nb.fit(train_hashed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions for train from our training set\n",
    "y_hat = nb.predict(train_hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3322714,), (3322714,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure we have the right number of predictions\n",
    "(y_hat.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5632392676589077"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring our train predictions. They should be close to 1. \n",
    "roc_auc_score(y_train, y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6895454334215533"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_train, nb.predict_proba(train_hashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [1751384 1571330]\n"
     ]
    }
   ],
   "source": [
    "# This shows that we guessed 0 for each one of our rows.\n",
    "unique, counts = np.unique(y_hat, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB did make guesses in both classes, but did not achieve very good results. Let's try a different Bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5666945755788791\n",
      "[0 1] [1766735 1555979]\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb = bnb.fit(train_hashed, y_train)\n",
    "y_hat_bnb = bnb.predict(train_hashed)\n",
    "print(roc_auc_score(y_train, y_hat_bnb))\n",
    "unique, counts = np.unique(y_hat_bnb, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "%time lr = lr.fit(train_hashed, y_train)\n",
    "y_hat_lr = lr.predict(train_hashed)\n",
    "print(roc_auc_score(y_train, y_hat_lr)) \n",
    "unique, counts = np.unique(y_hat_lr, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results, and we are now predicting both classes, but we're not doing much better than the baseline (.5).\n",
    "\n",
    "Let's try two more models on the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgboost()\n",
    "%time xg = xg.fit(train_hashed, y_train)\n",
    "y_hat_xg = xg.predict(train_hashed)\n",
    "print(roc_auc_score(y_train, y_hat_xg)) \n",
    "unique, counts = np.unique(y_hat_xg, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "%time rf = rf.fit(train_hashed, y_train)\n",
    "y_hat_rf = rf.predict(train_hashed)\n",
    "print(roc_auc_score(y_train, y_hat_rf)) \n",
    "unique, counts = np.unique(y_hat_rf, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "With undersampling of the majority class, we are able to see some positive impact on predictions from the FeatureHasher columns - however it is fairly minimal (+.05 Roc-Auc score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
