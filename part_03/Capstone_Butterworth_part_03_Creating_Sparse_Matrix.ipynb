{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook contains steps to create a sparse matrix of features from the training dataset\n",
    "# The id, device_id, device_ip and click columns are left out\n",
    "# Object columns are cast as categorical and one-hot-encoded, then joined to a sparse CSR matrix of numeric features\n",
    "# The resulting sparse matrix (~29k features x 40MM rows) has been stored as an .npz file in the /assets/ directory\n",
    "# Clicks from the training set have been stored in a CSV file called y_train\n",
    "# TODO (if needed) - check to make sure that correct index values have been persisted through transformations and joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data and Sample Submission\n",
    "\n",
    "Loading in our full test file as well as the sample submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in test data as well as the Sample Submission file\n",
    "sample = pd.read_csv('../assets/sampleSubmission')\n",
    "test = pd.read_csv('../assets/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "For this notebook we are only loading 20% of our full training dataset. This will allow us to experiment with feature engineering on a local device with 8GB of memory. Running the same operations against the full set may require a different solution (eg AWS Sagemaker or Databricks Spark Cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainaa loaded\n"
     ]
    }
   ],
   "source": [
    "# Because of GitHub space limits (no files over 2GB), train data file was split into 5 pieces\n",
    "\n",
    "# Loading the first file with header row to use for column names\n",
    "%time train = pd.read_csv(\"../assets/trainaa\")\n",
    "print(\"trainaa loaded\")\n",
    "\n",
    "# Loading subsequent files as separate dataframes with common column names\n",
    "#trainab = pd.read_csv(\"../assets/trainab\", header=None, names=trainaa.columns)\n",
    "#print(\"trainab loaded\")\n",
    "#trainac = pd.read_csv(\"../assets/trainac\", header=None, names=trainaa.columns)\n",
    "#print(\"trainac loaded\")\n",
    "#trainad = pd.read_csv(\"../assets/trainad\", header=None, names=trainaa.columns)\n",
    "#print(\"trainad loaded\")\n",
    "#trainae = pd.read_csv(\"../assets/trainae\", header=None, names=trainaa.columns)\n",
    "#print(\"trainae loaded\")\n",
    "\n",
    "# Concatenating all files into one training set for EDA purposes\n",
    "#%time train = pd.concat([trainaa, trainab, trainac, trainad, trainae], ignore_index=True)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a list of categorical columns\n",
    "# dropping device_id and device_ip columns (with 2MM+ and 6MM+ values) for now\n",
    "obj_cols_train = train.select_dtypes(include='object').drop(columns=['device_id',  'device_ip'])\n",
    "obj_cols_test = test.select_dtypes(include='object').drop(columns=['device_id',  'device_ip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id          object\n",
       "site_domain      object\n",
       "site_category    object\n",
       "app_id           object\n",
       "app_domain       object\n",
       "app_category     object\n",
       "device_model     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_cols_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "site_id          object\n",
       "site_domain      object\n",
       "site_category    object\n",
       "app_id           object\n",
       "app_domain       object\n",
       "app_category     object\n",
       "device_model     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_cols_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    878\u001b[0m             dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep,\n\u001b[1;32m    879\u001b[0m                                     \u001b[0mdummy_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_na\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m                                     drop_first=drop_first, dtype=dtype)\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0mwith_dummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dummies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0msp_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mndx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0;31m# Blank entries if not dummy_na and code == -1, #GH4446\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trying one colum at a time\n",
    "%time obj_dummies_train_1 = pd.get_dummies(train[['site_id', 'site_domain', 'site_category']], sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time obj_dummies_train_2 = pd.get_dummies(train[['app_id', 'app_domain', 'app_category', 'device_model']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dummies_test = pd.get_dummies(obj_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummies = obj_dummies_train_1.merge(obj_dummies_train_2, left_index=True, right_index=True)\n",
    "train_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%train dummies_aligned_train, dummies_aligned_test = obj_dummies_train.align(obj_dummies_test, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Label Encoding the object columns to make them into 1s and 0s\n",
    "\n",
    "#le_train = LabelEncoder()\n",
    "#le_test = LabelEncoder()\n",
    "\n",
    "#%time obj_cols_enc_train = obj_cols_train.apply(le_train.fit_transform)\n",
    "#obj_cols_enc_test = obj_cols_test.apply(le_test.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert red = blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting columns as categorical so we can generate sparse matrix\n",
    "obj_cols_train = obj_cols_train.apply(lambda x: x.astype('category'))\n",
    "obj_cols_test = obj_cols_test.apply(lambda x: x.astype('category'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shapes\n",
    "print(f\"Train object cols encoded shape: {obj_cols_enc_train.shape}\\n Test object cols encoded shape: {obj_cols_enc_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding the categorical columns\n",
    "\n",
    "ohe_train = OneHotEncoder()\n",
    "ohe_test = OneHotEncoder()\n",
    "\n",
    "%time onehot_train = ohe_train.fit_transform(obj_cols_enc_train)\n",
    "onehot_test = ohe_test.fit_transform(obj_cols_enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot returned a sparse matrix\n",
    "\n",
    "type(object_cols_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y (which we will need for validation)\n",
    "y = train.click.copy()\n",
    "\n",
    "# Test IDs (which we will need for sample submission)\n",
    "test_ids = test['id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "num_cols_train = train.select_dtypes(exclude='object').drop(columns=['id','click'])\n",
    "\n",
    "num_cols_test = test.select_dtypes(exclude='object').drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing numeric columns \n",
    "\n",
    "ss_train = StandardScaler()\n",
    "ss_test = StandardScaler()\n",
    "\n",
    "%time num_cols_scaled_train = ss_train.fit_transform(num_cols_train)\n",
    "num_cols_scaled_test = ss_test.fit_transform(num_cols_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a sparse matrix from the scaled numeric columns\n",
    "\n",
    "#%time num_sparse_train = sparse.csr_matrix(num_cols_scaled_train)\n",
    "#num_sparse_test = sparse.csr_matrix(num_cols_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape\n",
    "\n",
    "#numeric_sparse.shape#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling a complete sparse matrix from numeric and dummy features\n",
    "\n",
    "#%time sparse_features = sparse.hstack([numeric_sparse, object_cols_onehot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape - ~1 Trillion \"cells\" represented\n",
    "#sparse_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sparse matrix to a file\n",
    "\n",
    "#%time saved = sparse.save_npz('../assets/sparse_no_dev_id_dev_ip', sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y to a file\n",
    "\n",
    "#%time y_saved = y.to_csv('../assets/y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
