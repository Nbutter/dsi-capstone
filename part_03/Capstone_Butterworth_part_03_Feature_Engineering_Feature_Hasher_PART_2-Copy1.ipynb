{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering With Reduced Dataset\n",
    "\n",
    "(Original version of notebook)\n",
    "\n",
    "Since feature engineering for categorical variables (eg one hot encoding) is difficult with our full 6GB dataset on our local computer (8GB RAM), in this notebook we load ~25% of our training data (~10MM rows) to explore different techniques, such as Feature Hashing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction import FeatureHasher \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report, log_loss, roc_auc_score, roc_curve\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Data and Sample Submission\n",
    "\n",
    "Loading in our full test file as well as the sample submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in test data as well as the Sample Submission file\n",
    "sample = pd.read_csv('../assets/sampleSubmission')\n",
    "test = pd.read_csv('../assets/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Partial Training Data\n",
    "\n",
    "For this notebook we are only loading 20% of our full training dataset (~1.5GB, ~10MM rows). This will allow us to experiment with feature engineering on a local device with 8GB of memory. Running the same operations against the full set may require a different solution (eg AWS Sagemaker or Databricks Spark Cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 s, sys: 10.7 s, total: 51.5 s\n",
      "Wall time: 53.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "      <th>hour</th>\n",
       "      <th>C1</th>\n",
       "      <th>banner_pos</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_domain</th>\n",
       "      <th>site_category</th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_domain</th>\n",
       "      <th>...</th>\n",
       "      <th>device_type</th>\n",
       "      <th>device_conn_type</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000009e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>14102100</td>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>1fbe01fe</td>\n",
       "      <td>f3845767</td>\n",
       "      <td>28905ebd</td>\n",
       "      <td>ecad2386</td>\n",
       "      <td>7801e8d9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15706</td>\n",
       "      <td>320</td>\n",
       "      <td>50</td>\n",
       "      <td>1722</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  click      hour    C1  banner_pos   site_id site_domain  \\\n",
       "0  1.000009e+18      0  14102100  1005           0  1fbe01fe    f3845767   \n",
       "\n",
       "  site_category    app_id app_domain ...  device_type device_conn_type    C14  \\\n",
       "0      28905ebd  ecad2386   7801e8d9 ...            1                2  15706   \n",
       "\n",
       "   C15  C16   C17  C18  C19  C20  C21  \n",
       "0  320   50  1722    0   35   -1   79  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because of GitHub space limits (no files over 2GB), train data file was split into 5 pieces\n",
    "\n",
    "# Loading the first file with header row to use for column names\n",
    "%time train = pd.read_csv(\"../assets/trainaa\")\n",
    "\n",
    "# Checking the columns present\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Y Train\n",
    "We'll use the \"click\" column in the training set as our target or \"y\" series for training purposes, dropping it from our training dataframe.  We will also remove the \"id\" column, which does not add value for training purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a y_train with our click information\n",
    "y_train = train.click\n",
    "\n",
    "# From the Kaggle site, we know 'id' is just a record indicator, so we can drop it\n",
    "# along with the \"click\" column, which is our target\n",
    "train = train.drop(columns=['click', 'id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9999999, 22), (4577464, 23))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking our shapes. We see that \"test\" has an extra column, because it still has \"id\". We'll drop this column next \n",
    "# after using it to prepare a submission dataframe, where we'll put our predictions\n",
    "(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping a Submission Dataframe\n",
    "We take the \"id\" column and index from the \"test\" dataframe and add a \"click\" column, following the model of the sample submission file.  For now the click column is filled with zeros; later as we generate model predictions we will place them here, before saving this dataframe out as a csv to upload to Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000174058809263569</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  click\n",
       "0  10000174058809263569    0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the columns of our submission sample\n",
    "sample.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        uint64\n",
       "click    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the datatypes of our submission sample\n",
    "sample.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        uint64\n",
       "click    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the submission dataframe and conforming types\n",
    "submit = pd.DataFrame(test.id, index=test.index)\n",
    "submit['click'] = 0.0\n",
    "submit['id'] = submit['id'].astype(np.uint64)\n",
    "\n",
    "# Verifying that our new submission has the correct datatypes\n",
    "submit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000174058809264128</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  click\n",
       "0  10000174058809264128    0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that our new submission has the correct columns\n",
    "submit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9999999, 22), (4577464, 22), (4577464, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the id column from test\n",
    "test = test.drop(columns=\"id\")\n",
    "\n",
    "# Now we can verify that \"train\" and \"test\" have the same number of columns, as expected\n",
    "# We can also verify that the \"submit\" dataframe has the same number of rows as \"test\"\n",
    "\n",
    "(train.shape, test.shape, submit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Date Columns\n",
    "We have a column representing hour/day/month/year for each hour in our 14 day sample (\"hour\"). We will convert this to a date-time object, then add feature columns based on the day of the week and the hour of the day.  Finally we will remove the hour column. We'll perform the same operation on both our \"train\" and \"test\" dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make day-of-week, hour-of-day features\n",
    "def make_date_features(dataframe, frame_name=\"dataframe\", date_col=\"hour\"): \n",
    "    date_obj = pd.to_datetime(dataframe[date_col])\n",
    "    \n",
    "    dataframe['hour-of-the-day'] = date_obj.dt.hour\n",
    "    print(f\"Created 'hour-of-the-day' column in {frame_name}\")\n",
    "    \n",
    "    dataframe['day-of-the-week'] = date_obj.dt.dayofweek\n",
    "    print(f\"Created 'day-of-the-week' column in {frame_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'hour-of-the-day' column in train\n",
      "Created 'day-of-the-week' column in train\n",
      "Created 'hour-of-the-day' column in test\n",
      "Created 'day-of-the-week' column in test\n"
     ]
    }
   ],
   "source": [
    "# running \"train\" and \"test\" through our date features function to create the engineered columns \n",
    "make_date_features(train, \"train\", \"hour\")\n",
    "make_date_features(test, \"test\", \"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the \"hour\" column now that we no longer need it\n",
    "train = train.drop(columns=[\"hour\"])\n",
    "test = test.drop(columns=[\"hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from Adam - Standup July 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator object from readcsv - ton of cool features - get batches\n",
    "# hashing vectorizer\n",
    "# stochastic sgd\n",
    "# no SVM - no KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Hash Experiment\n",
    "\n",
    "We have multiple columns with large numbers of categorical values. Let's see if we can featureize them in memory with the FeatureHash class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>device_ip</td>\n",
       "      <td>2129661</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>device_id</td>\n",
       "      <td>786740</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>device_model</td>\n",
       "      <td>6863</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>app_id</td>\n",
       "      <td>5469</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>site_domain</td>\n",
       "      <td>4585</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>site_id</td>\n",
       "      <td>3496</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C14</td>\n",
       "      <td>1030</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>app_domain</td>\n",
       "      <td>390</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C17</td>\n",
       "      <td>226</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C20</td>\n",
       "      <td>168</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C19</td>\n",
       "      <td>47</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C21</td>\n",
       "      <td>42</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>app_category</td>\n",
       "      <td>33</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>site_category</td>\n",
       "      <td>23</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C16</td>\n",
       "      <td>9</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C15</td>\n",
       "      <td>8</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>banner_pos</td>\n",
       "      <td>7</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C1</td>\n",
       "      <td>7</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>device_type</td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C18</td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>device_conn_type</td>\n",
       "      <td>4</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hour-of-the-day</td>\n",
       "      <td>1</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>day-of-the-week</td>\n",
       "      <td>1</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column Name Unique Values   Dtype\n",
       "0          device_ip       2129661  object\n",
       "1          device_id        786740  object\n",
       "2       device_model          6863  object\n",
       "3             app_id          5469  object\n",
       "4        site_domain          4585  object\n",
       "5            site_id          3496  object\n",
       "6                C14          1030   int64\n",
       "7         app_domain           390  object\n",
       "8                C17           226   int64\n",
       "9                C20           168   int64\n",
       "10               C19            47   int64\n",
       "11               C21            42   int64\n",
       "12      app_category            33  object\n",
       "13     site_category            23  object\n",
       "14               C16             9   int64\n",
       "15               C15             8   int64\n",
       "16        banner_pos             7   int64\n",
       "17                C1             7   int64\n",
       "18       device_type             4   int64\n",
       "19               C18             4   int64\n",
       "20  device_conn_type             4   int64\n",
       "21   hour-of-the-day             1   int64\n",
       "22   day-of-the-week             1   int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the number of unique values in each column of the training dataset\n",
    "uniques = pd.DataFrame(data=[train.columns.values, [len(train[col].unique()) for col in train.columns], [train[col].dtype for col in train.columns]]).T\n",
    "uniques = uniques.rename({0:'Column Name', 1:'Unique Values', 2:'Dtype'}, axis=1)\n",
    "uniques = uniques.sort_values(by='Unique Values', ascending=False).reset_index(drop=True)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we are going to let FeatureHasher choose the number of features vs setting them ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5fa91dd9f9b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's try this out with \"device model\" first - only about 7k values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfh_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureHasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_negative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# so we can use NaiveBayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit = fh_1.fit_transform(train.device_model)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_features'"
     ]
    }
   ],
   "source": [
    "# Let's try this out with \"device model\" first - only about 7k values\n",
    "fh_1 = FeatureHasher(num_features=uniques.iloc[2, 1], input_type='string', non_negative=True) # so we can use NaiveBayes\n",
    "%time fit = fh_1.fit_transform(train.device_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"Fit shape: {fit.shape}, Fit non-nulls: {fit.nnz}\")\n",
    "print(f\"Non-null fraction of total: {'{:.10f}'.format(fit.nnz/(fit.shape[0] * fit.shape[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we'll try \"device_id\" - about 700k values\n",
    "fh_2 = FeatureHasher(num_features=uniques.iloc[1, 1], input_type='string', non_negative=True)\n",
    "%time fit2 = fh_2.fit_transform(train.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fit2 shape: {fit2.shape}, Fit2 non-nulls: {fit2.nnz}\")\n",
    "print(f\"Non-null fraction of total: {'{:.10f}'.format(fit2.nnz/(fit2.shape[0] * fit2.shape[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we'll try \"device_ip\" - 2MM values in the reduced training set\n",
    "fh_3 = FeatureHasher(num_features=uniques.iloc[0, 1], input_type='string', non_negative=True)\n",
    "%time fit3 = fh_3.fit_transform(train.device_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fit3 shape: {fit3.shape}, Fit3 non-nulls: {fit3.nnz}\")\n",
    "print(f\"Non-null fraction of total: {'{:.10f}'.format(fit3.nnz/(fit3.shape[0] * fit3.shape[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the same objects on test now that they have been fitted on train\n",
    "fit1_test = fh_1.transform(test.device_model)\n",
    "fit2_test = fh_2.transform(test.device_id)\n",
    "fit3_test = fh_3.transform(test.device_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that all our shapes are as expected\n",
    "print((fit.shape, fit1_test.shape))\n",
    "print((fit2.shape, fit2_test.shape))\n",
    "print((fit3.shape, fit3_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use with a numeric series (eg C14) we need to convert the values to strings\n",
    "# Note that casting the series as type \"object\" won't work - we need each value to be parsed as a string\n",
    "fh_4 = FeatureHasher(num_features=uniques.iloc[6, 1], input_type='string', non_negative=True)\n",
    "%time fit4 = fh_4.fit_transform(train.C14.map(lambda x: str(x)))\n",
    "%time fit4_test = fh_4.transform(test.C14.map(lambda x: str(x)))\n",
    "print((fit4.shape, fit4_test.shape))\n",
    "print(fit4.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models on FeatureHash Columns\n",
    "At this point, we have used FeatureHash to convert four columns of each dataset into sparse feature matrices, with identical width (ie number of columns). Therefore we are in a position to use these features for training and predicting on a model which takes sparse features. Let's do that for demonstration purposes. \n",
    "\n",
    "If successful, we can go on to create pipelines for transforming all of our columns and move on to the modeling stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the Multinomial Naive Bayes classifier, since each of our features may have a multinomial distribution\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shapes to make sure our matrices are congruent\n",
    "(fit.shape, fit2.shape, fit3.shape, fit4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling one big sparse matrix from the different HashFeature outputs\n",
    "%time train_4 = hstack((fit, fit2, fit3, fit4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling our sparse matrix\n",
    "ss = StandardScaler(with_mean=False) # to maintain sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_4 = ss.fit_transform(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that we have compatible shapes of correct dimensionality\n",
    "(train_4.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model \n",
    "%time nb.fit(train_4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions for train from our training set\n",
    "y_hat = nb.predict(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we have the right number of predictions\n",
    "(y_hat.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring our train predictions. They should be close to 1. \n",
    "roc_auc_score(y_train, y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Something's wrong - this is the baseline...let's look at log loss (should be under 1)\n",
    "log_loss(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows that we guessed 0 for each one of our rows.\n",
    "unique, counts = np.unique(y_hat, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB didn't work with our sparse matrix. Our model guessed all 0s. Let's try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lr = lr.fit(train_4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lr = lr.predict(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train, y_hat_lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_hat, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same issue. Our models are guessing all 0s - letting FeatureHasher choose the number of components makes no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Dimensionality Reduction First\n",
    "As a hypothesis, maybe our matrix is simply too big, with too many features (2.9MM). Let's try reducing dimensionality and see whether that helps us get better results after training new models. Since PCA doesn't accept sparse inputs, we'll try this using the \"Truncated SVD\" class from SK Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try 25 components\n",
    "# cf https://arxiv.org/abs/1305.5870\n",
    "tsvd = TruncatedSVD(n_components=25) #  try algorithm='arpack' to keep from crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting our reducer to the full dataset\n",
    "# NOTE: takes 18 minutes on my local 8GB machine\n",
    "%time tsvd = tsvd.fit(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing our \"train_4\" sparse matrix which has already been scaled\n",
    "# this crashes the kernel at 1000 and 100 components...\n",
    "%time train_4_reduced = tsvd.transform(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shapes before fitting a model\n",
    "train_4_reduced.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have negative values, so we need one more transform before we can run MultinomialNB\n",
    "mmx = MinMaxScaler()\n",
    "%time train_4_reduced = mmx.fit_transform(train_4_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2 = MultinomialNB()\n",
    "%time nb2 = nb2.fit(train_4_reduced, y_train)\n",
    "%time y_hat_nb2 = nb2.predict(train_4_reduced)\n",
    "print(roc_auc_score(y_train, y_hat_nb2))\n",
    "print(log_loss(y_train, y_hat_nb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression()\n",
    "%time lr2 = lr2.fit(train_4_reduced, y_train)\n",
    "%time y_hat_lr2 = lr2.predict(train_4_reduced)\n",
    "print(roc_auc_score(y_train, y_hat_lr2))\n",
    "print(log_loss(y_train, y_hat_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_hat_lr2, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results. Our models are simply guessing 0 for every row despite dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We have shown that while we can use FeatureHasher to generate sparse matrices from our high-volume categorical features, the resulting matrices are not presently useful for training (2018-07-05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
